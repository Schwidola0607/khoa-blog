<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>Sequence Parallelism for Inference: Part 1 | My New Hugo Site</title>
<meta name="title" content="Sequence Parallelism for Inference: Part 1" />
<meta name="description" content="Introduction
Nd&#43; parallelism techniques are often invented for training first, then adapted for inference. Most techniques have seen great success in today&rsquo;s inference world (both individually and together). However, Sequence Parallelism has not seen wide adoption in the open source world. If you look at vLLM or SGLang today, sequence parallelism is either in not-so-active development or completely deprioritized.
As a huge fan of low time-to-first-token (TTFT) long-sequence inference (motivated by my work at Augment Code‚Äîcheck us out!), this greatly saddens me. This blog series aims to bring back some love for Sequence Parallelism for Inference." />
<meta name="keywords" content="" />


<meta property="og:url" content="https://example.org/blog/sequence_parallelism_for_inference_part_1/">
  <meta property="og:site_name" content="My New Hugo Site">
  <meta property="og:title" content="Sequence Parallelism for Inference: Part 1">
  <meta property="og:description" content="Introduction Nd&#43; parallelism techniques are often invented for training first, then adapted for inference. Most techniques have seen great success in today‚Äôs inference world (both individually and together). However, Sequence Parallelism has not seen wide adoption in the open source world. If you look at vLLM or SGLang today, sequence parallelism is either in not-so-active development or completely deprioritized. As a huge fan of low time-to-first-token (TTFT) long-sequence inference (motivated by my work at Augment Code‚Äîcheck us out!), this greatly saddens me. This blog series aims to bring back some love for Sequence Parallelism for Inference.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="blog">
    <meta property="article:published_time" content="2025-12-23T21:35:22+00:00">
    <meta property="article:modified_time" content="2025-12-23T21:35:22+00:00">




  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Sequence Parallelism for Inference: Part 1">
  <meta name="twitter:description" content="Introduction Nd&#43; parallelism techniques are often invented for training first, then adapted for inference. Most techniques have seen great success in today‚Äôs inference world (both individually and together). However, Sequence Parallelism has not seen wide adoption in the open source world. If you look at vLLM or SGLang today, sequence parallelism is either in not-so-active development or completely deprioritized. As a huge fan of low time-to-first-token (TTFT) long-sequence inference (motivated by my work at Augment Code‚Äîcheck us out!), this greatly saddens me. This blog series aims to bring back some love for Sequence Parallelism for Inference.">




  <meta itemprop="name" content="Sequence Parallelism for Inference: Part 1">
  <meta itemprop="description" content="Introduction Nd&#43; parallelism techniques are often invented for training first, then adapted for inference. Most techniques have seen great success in today‚Äôs inference world (both individually and together). However, Sequence Parallelism has not seen wide adoption in the open source world. If you look at vLLM or SGLang today, sequence parallelism is either in not-so-active development or completely deprioritized. As a huge fan of low time-to-first-token (TTFT) long-sequence inference (motivated by my work at Augment Code‚Äîcheck us out!), this greatly saddens me. This blog series aims to bring back some love for Sequence Parallelism for Inference.">
  <meta itemprop="datePublished" content="2025-12-23T21:35:22+00:00">
  <meta itemprop="dateModified" content="2025-12-23T21:35:22+00:00">
  <meta itemprop="wordCount" content="1464">
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  :root {
    --width: 720px;
    --font-main: Verdana, sans-serif;
    --font-secondary: Verdana, sans-serif;
    --font-scale: 1em;
    --background-color: #fff;
    --heading-color: #222;
    --text-color: #444;
    --link-color: #3273dc;
    --visited-color: #8b6fcb;
    --blockquote-color: #222;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #01242e;
      --heading-color: #eee;
      --text-color: #ddd;
      --link-color: #8cc2dd;
      --visited-color: #8b6fcb;
      --blockquote-color: #ccc;
    }
  }

  body {
    font-family: var(--font-secondary);
    font-size: var(--font-scale);
    margin: auto;
    padding: 20px;
    max-width: var(--width);
    text-align: left;
    background-color: var(--background-color);
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: var(--text-color);
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    font-family: var(--font-main);
    color: var(--heading-color);
  }

  a {
    color: var(--link-color);
    cursor: pointer;
    text-decoration: none;
  }

  a:hover {
    text-decoration: underline;
  }

  nav a {
    margin-right: 8px;
  }

  strong,
  b {
    color: var(--heading-color);
  }

  button {
    margin: 0;
    cursor: pointer;
  }

  time {
    font-family: monospace;
    font-style: normal;
    font-size: 15px;
  }

  main {
    line-height: 1.6;
  }

  table {
    width: 100%;
  }

  hr {
    border: 0;
    border-top: 1px dashed;
  }

  img {
    max-width: 100%;
  }

  code {
    font-family: monospace;
    padding: 2px;
    border-radius: 3px;
  }

  blockquote {
    border-left: 1px solid #999;
    color: var(--blockquote-color);
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px 0;
    text-align: center;
  }

  .title:hover {
    text-decoration: none;
  }

  .title h1 {
    font-size: 1.5em;
  }

  .inline {
    width: auto !important;
  }

  .highlight,
  .code {
    border-radius: 3px;
    margin-block-start: 1em;
    margin-block-end: 1em;
    overflow-x: auto;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 130px;
  }

  ul.blog-posts li a:visited {
    color: var(--visited-color);
  }

</style>

  <link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css"
  integrity="sha384-WcoG4HRXMzYzfCgiyfrySxx90XSl2rxY5mnVY5TwtWE6KLrArNKn0T/mOgNL0Mmi"
  crossorigin="anonymous"
>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js"
  integrity="sha384-J+9dG2KMoiR9hqcFao0IBLwxt6zpcyN68IgwzsCSkbreXUjmNVRhPFTssqdSGjwQ"
  crossorigin="anonymous">
</script>
<script
  defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js"
  integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh"
  crossorigin="anonymous">
</script>
<script>
  document.addEventListener("DOMContentLoaded", function() {
    
    if (typeof renderMathInElement !== 'undefined') {
      renderMathInElement(document.body, {
        delimiters: [
          {left: '\\[', right: '\\]', display: true},   
          {left: '$$', right: '$$', display: true},     
          {left: '\\(', right: '\\)', display: false},  
          {left: '$', right: '$', display: false},      
        ],
        throwOnError: false,
        strict: false
      });
    } else {
      
      setTimeout(function() {
        if (typeof renderMathInElement !== 'undefined') {
          renderMathInElement(document.body, {
            delimiters: [
              {left: '\\[', right: '\\]', display: true},
              {left: '$$', right: '$$', display: true},
              {left: '\\(', right: '\\)', display: false},
              {left: '$', right: '$', display: false},
            ],
            throwOnError: false,
            strict: false
          });
        }
      }, 100);
    }
  });
</script>




<style>
   
  :root {
    --background-color: #fafafa;
    --heading-color: #2c3e50;
    --text-color: #34495e;
    --link-color: #3498db;
    --visited-color: #9b59b6;
    --blockquote-color: #555;
    --code-background: #f4f4f4;
    --code-border: #e0e0e0;
  }

  @media (prefers-color-scheme: dark) {
    :root {
      --background-color: #1a1a1a;
      --heading-color: #e8e8e8;
      --text-color: #d0d0d0;
      --link-color: #5dade2;
      --visited-color: #bb8fce;
      --blockquote-color: #b0b0b0;
      --code-background: #2a2a2a;
      --code-border: #404040;
    }
  }

   
  .katex {
    font-size: 1.1em;
    color: var(--text-color);
  }

  .katex-display {
    margin: 1.5em 0;
    overflow-x: auto;
    overflow-y: hidden;
    text-align: center;
  }

  .katex-display > .katex {
    text-align: left;
    display: inline-block;
    max-width: 100%;
  }

   
  .katex-html {
    white-space: nowrap;
  }

   
  @media (prefers-color-scheme: dark) {
    .katex {
      color: #e8e8e8;
    }
    .katex .base {
      color: #e8e8e8;
    }
  }

   
  code {
    background-color: var(--code-background);
    border: 1px solid var(--code-border);
    padding: 2px 6px;
  }

  pre code {
    background-color: var(--code-background);
    border: 1px solid var(--code-border);
    padding: 10px;
    display: block;
    overflow-x: auto;
  }

   
  blockquote {
    border-left: 3px solid var(--link-color);
    background-color: var(--code-background);
    padding: 10px 20px;
    margin: 1em 0;
  }

   
  figure {
    margin: 1.5em 0;
  }

  figcaption {
    font-size: 0.9em;
    color: var(--text-color);
    opacity: 0.8;
    margin-top: 0.5em;
  }

   
  ul, ol {
    padding-left: 1.5em;
  }

  li {
    margin: 0.5em 0;
  }

   
  p {
    margin: 1em 0;
  }

   
  h1, h2, h3, h4, h5, h6 {
    margin-top: 1.5em;
    margin-bottom: 0.5em;
  }

  h1 {
    border-bottom: 2px solid var(--code-border);
    padding-bottom: 0.3em;
  }

  h2 {
    border-bottom: 1px solid var(--code-border);
    padding-bottom: 0.2em;
  }
</style>

</head>

<body>
  <header><a href="/" class="title">
  <h2>My New Hugo Site</h2>
</a>
<nav>
<a href="/blog/">Blog</a>

</nav>
</header>
  <main>

<h1>Sequence Parallelism for Inference: Part 1</h1>
<p>
  <i>
    <time datetime='2025-12-23'>
      23 Dec, 2025
    </time>
  </i>
</p>

<content>
  <h1 id="introduction">Introduction</h1>
<p><a href="https://blog.ezyang.com/2025/08/the-parallelism-mesh-zoo/">Nd+ parallelism techniques</a> are often invented for training first, then adapted for inference. Most techniques have seen great success in today&rsquo;s inference world (both individually and together). However, Sequence Parallelism has not seen wide adoption in the open source world. If you look at vLLM or SGLang today, sequence parallelism is either in not-so-active development or completely deprioritized.
As a huge fan of low time-to-first-token (TTFT) long-sequence inference (motivated by my work at <a href="https://www.augmentcode.com/">Augment Code</a>‚Äîcheck us out!), this greatly saddens me. This blog series aims to bring back some love for Sequence Parallelism for Inference.</p>
<blockquote>
<p>üíî Another embarrassing reason for this blog series is because I have spent ungodly hours re-inventing Sequence Parallelism variants and begrudgingly realized that many of my so-called original ideas were just from papers I read back in school (many such cases). So I decided to sit down and write to prevent another poor soul (and my future self) from wasting time re-deriving the basics of Sequence Parallelism</p>
</blockquote>
<p>This blog series assumes that model weights can fit on a small number of GPUs within a single node (2 or 4). If you need all 8 GPUs in a node or multi-node deployment just to fit the weights, Sequence Parallelism might not be the sole solution - you may need to use it in conjunction with other parallelism techniques.</p>
<h2 id="notation">Notation</h2>
<ul>
<li>
<p>$N$ is the tensor size</p>
</li>
<li>
<p>$p$ is the number of devices</p>
</li>
<li>
<p>$B$ is batch size</p>
</li>
<li>
<p>$T$ is sequence length</p>
</li>
<li>
<p>$nkv \text{ and } nq$ denote number of kv heads and q heads respectively.</p>
</li>
<li>
<p>$d$ is headdim</p>
</li>
<li>
<p>$H$ is hidden dimension, $H = nq \times d$</p>
</li>
</ul>
<h1 id="what-is-sequence-parallelism-sp">What is Sequence Parallelism (SP)?</h1>
<p>In one sentence: sequence parallelism shards the input along the sequence dimension across devices, and subsequent operations‚Äîsuch as ResidualAdd, LayerNorm, and MatMuls‚Äîare performed on these partitioned sequences. Attention is the only exception, where tokens are dependent (causal) and often require either full KV and/or full Q for computation.
The idea sounds simple enough, but in reality there are <em>many</em> sequence parallelism variants. Before we dive deeper into different SP variants, let&rsquo;s take a quick detour to explain why Tensor Parallelism isn&rsquo;t good enough for low-latency prefill.</p>
<h1 id="why-is-tensor-parallelism-tp-not-good-enough">Why is Tensor Parallelism (TP) not good enough?</h1>
<p>To reduce TTFT, the first thought is to apply a higher degree of TP. During prefill, as we increase TP degree, both attention computation and MLP scale down roughly linearly. However:</p>
<ol>
<li>
<p>LayerNorm computation does not scale with more devices</p>
</li>
<li>
<p>All-Reduce communication volume increases with more devices</p>
</li>
</ol>
<p><strong>For latency-sensitive applications, this is simply not good enough. The solution is, as you&rsquo;ve guessed from the title, Sequence Parallelism (and friends)!</strong></p>
<h1 id="sequence-parallelism-variants">Sequence Parallelism variants</h1>
<p>There are many SP variants. I&rsquo;ve created this semi-opinionated taxonomy to organize them.</p>
<p><img src="/SP_taxonomy.png" alt="SP Taxonomy">
As shown in the taxonomy, we&rsquo;ll focus on Vanilla SP, Megatron-style SP, and Ulysses SP.</p>
<h2 id="vanilla-sequence-parallelism-or-context-parallelism">Vanilla Sequence Parallelism (or Context Parallelism)</h2>
<p><img src="/SP.png" alt="Vanilla Sequence Parallelism">
<strong>Pros</strong>: With Vanilla Sequence Parallelism, MLP and LN scale down roughly linearly with the number of devices since these operations now act on sharded input sequences.
Furthermore, the communication overhead is significantly lower than TP:</p>
<ul>
<li>
<p>We only need 1 All-Gather compared to 2 All-Reduces</p>
</li>
<li>
<p>For Ring All-Gather, the per-device communication volume is $\frac{2 \times (p - 1) \times N }{p}$</p>
</li>
<li>
<p>However, in this case, $N = B \times T \times 2nkv \times d$. For GQA, $2nkv \times d << H = nq \times d$</p>
</li>
</ul>
<p><strong>Cons</strong>: Vanilla Sequence Parallelism introduces attention computation imbalance. The rightmost portion of Q must attend to <em>all</em> of KV, while the leftmost portion of Q only attends to its corresponding leftmost portion of KV. As a result, Attention still scales with more devices but performs slightly worse than TP.
You can use load balancing algorithms to combat the attention imbalance. For strictly causal masks, one notable approach is zig-zag attention.</p>
<div style="display: flex; gap: 20px; justify-content: center;">
  <figure style="flex: 1; text-align: center;">
    <img src="/contiguous_sharding.png" alt="Contiguous sharding" style="max-width: 100%;">
    <figcaption>Contiguous sharding</figcaption>
  </figure>
  <figure style="flex: 1; text-align: center;">
    <img src="/zigzag_sharding.png" alt="Zig-zag sharding" style="max-width: 100%;">
    <figcaption>Zig-zag sharding</figcaption>
  </figure>
</div>
Approaches like this have a couple of pain points: (i) NCCL all-gather expects a contiguous tensor, and (ii) for more generalized masks (especially document masking), the sharding layout becomes quite complex.
<h2 id="megatron-sequence-parallelism">Megatron Sequence Parallelism</h2>
<p><img src="/megatron.png" alt="Megatron Sequence Parallelism">
The high-level idea: perform Attention and MLP blocks in TP, and LayerNorm in SP. In detail:</p>
<ol>
<li>
<p>Shard the input sequence along the sequence dimension</p>
</li>
<li>
<p>Attention LayerNorm acts on the sharded input sequence</p>
</li>
<li>
<p>All-Gather the sharded input sequence into the full sequence</p>
</li>
<li>
<p>Perform the Attention Block (QKV proj, Attention, and O proj) in traditional TP fashion</p>
</li>
<li>
<p>Reduce partial results of O proj and scatter them along the input sequence</p>
</li>
<li>
<p>MLP LayerNorm acts on the sharded input sequence</p>
</li>
<li>
<p>All-Gather the sharded input sequence into the full sequence</p>
</li>
<li>
<p>Perform the MLP Block (expand, activation, and shrink) in traditional TP fashion</p>
</li>
<li>
<p>Reduce partial results of shrink and scatter them along the input sequence for the next layer&rsquo;s Attention LayerNorm</p>
</li>
</ol>
<blockquote>
<p>‚ùì Uhh why would one use this over TP? Isn‚Äôt the point of Megatron-SP is to reduce activation memory overhead during training?</p>
</blockquote>
<p>Yes, but surprisingly this is <em>faster</em> than TP for prefill. While the communication overhead is roughly the same‚ÄîAll-Reduce is equivalent to All-Gather + Reduce-Scatter (relatively true for large message sizes, assuming the same tensor size)‚ÄîLN scales linearly since it acts on sharded input sequences.</p>
<ul>
<li>
<p>Ring All-Gather and Reduce-Scatter both have per-device communication volume of $\frac{(p - 1) \times N}{p}$. In practice, Reduce-Scatter is usually slightly slower than All-Gather.</p>
</li>
<li>
<p>As noted earlier, ring All-Reduce per-device communication volume is $\frac{2 \times (p - 1) \times N}{p}$, which is exactly All-Gather + Reduce-Scatter. In practice, ring All-Reduce for large message sizes is often implemented as a two-phase algorithm: Reduce-Scatter followed by All-Gather.</p>
</li>
</ul>
<p>Since we don&rsquo;t care about reducing activation memory, we can do even better for inference by eliminating one pair of All-Gather + Reduce-Scatter.</p>
<p><img src="/megatron-sp-tp.png" alt="Megatron SP-TP">
Instead of All-Gathering the sharded sequence after MLP-LN, we skip it and perform the MLP block on the sharded sequence. This eliminates a pair of All-Gather + Reduce-Scatter, cutting our communication overhead in half.
<strong>Pros:</strong> MLP, LN, and Attention all scale down ~linearly with the number of devices. Attention is head-parallel (no imbalance), while MLP and LN act on sharded input sequences. The communication overhead is ~1/2 that of TP‚Äîwe only do one All-Gather + Reduce-Scatter compared to two All-Reduces.
<strong>Cons:</strong> Compared to Vanilla SP, Megatron SP has significantly higher communication volume</p>
<ul>
<li>
<p>For the Megatron variant above, we do an All-Gather after Attn LN and a Reduce-Scatter after O proj, both with tensor sizes of $N = B \times T \times H$.</p>
</li>
<li>
<p>Once again, because $nq >> nkv$ for GQA models, per device communication volume of Megatron SP is significantly higher than that of Vanilla SP.</p>
</li>
</ul>
<h2 id="ulysses-sequence-parallelism">Ulysses Sequence Parallelism</h2>
<p><img src="/ulysses.png" alt="Ulysses Sequence Parallelism">
The high-level idea: perform only Attention computation in TP and everything else in SP. In detail:</p>
<ol>
<li>
<p>Shard the input sequence along the sequence dimension</p>
</li>
<li>
<p>Attention LayerNorm acts on the sharded input sequence</p>
</li>
<li>
<p>Pass the sharded input sequence to QKV proj to get sequence-sharded Q, K, V</p>
</li>
<li>
<p>Perform an All-To-All to reshard the sequence-sharded Q, K, V into head-sharded Q, K, V</p>
</li>
<li>
<p>Each device performs attention on a subset of heads</p>
</li>
<li>
<p>Perform an All-To-All to reshard the head-sharded <code>attn_output</code> back to sequence-sharded</p>
</li>
<li>
<p>Pass the sequence-sharded output to O proj, LayerNorm, and MLP block</p>
</li>
</ol>
<p><strong>Pros:</strong> MLP, LN, and Attention all scale linearly with the number of devices since Attention is head-parallel (no imbalance). The communication overhead is significantly smaller than TP‚Äîtwo All-To-Alls during Ulysses SP compared to two All-Reduces during TP.</p>
<ul>
<li>
<p>The per-device All-To-All communication volume is $N$. In practice, the self-data transfer step (where a device sends $\frac{N}{p}$ data to itself) is very fast because it&rsquo;s just a local copy.</p>
</li>
<li>
<p>For Ulysses, we perform an All-To-All after QKV projection where $N = \frac{B \times T \times (nq + nkv) \times d}{p}$ and another All-To-All where $N = \frac{B \times T \times H}{p}$.</p>
</li>
<li>
<p>Notice that per-device communication volume scales down roughly linearly with more devices. At $p = 4$, the ratio between Ulysses SP and TP is $\frac{2nq + nkv}{12nq}$. For GQA where $nkv << nq$, this ratio is <em>much</em> smaller than 1!</p>
</li>
</ul>
<p><strong>Cons:</strong> Ulysses cannot shard the sequence dimension beyond the number of KV heads, which is a deal breaker for MHA models where $nkv = 1$ or for GQA models where $nkv$ is substantially small.
Ulysses has slightly higher communication overhead than Vanilla SP.</p>
<ul>
<li>The per-device communication volume ratio between Ulysses and Vanilla SP is $\frac{2nq + nkv}{8nkv}$. For GQA with small $nq / nkv$, this ratio is only slightly larger than 1!</li>
</ul>
<h2 id="some-closing-notes">Some closing notes</h2>
<p>So far, it&rsquo;s been really fun to see how you can move communication between Transformer ops, switch between different sharding strategies, and shave off communication overhead. The computation and communication nature then change drastically. Notice how in all of these sequence parallelism variants, communication and computation are still done separately, the next post will be about fusing and overlapping them together! Stay tuned.</p>
<h3 id="references">References</h3>
<p><a href="https://arxiv.org/abs/2205.05198">Megatron-SP</a>
<a href="https://arxiv.org/pdf/2507.07120">Helix Parallelism</a>
<a href="https://arxiv.org/abs/2310.01889">Ring Attention</a>
<a href="https://www.snowflake.com/en/engineering-blog/ulysses-low-latency-llm-inference/">Arctic Ulysses</a>
<a href="https://www.snowflake.com/en/engineering-blog/arctic-inference-shift-parallelism/">Shift Parallelism</a>
<a href="https://arxiv.org/pdf/2411.01783v3">Context Parallelism</a></p>

</content>



<p>
  
</p>

  </main>
  <footer>Made with <a href="https://github.com/janraasch/hugo-bearblog/">Hugo  ï‚Ä¢·¥•‚Ä¢ î Bear</a>
</footer>

  
</body>

</html>
